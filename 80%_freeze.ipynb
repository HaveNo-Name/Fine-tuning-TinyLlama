{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f5e0b53",
   "metadata": {},
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import Dataset\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float32  # Use torch.float16 if training with fp16\n",
    ")\n",
    "model.to(\"cuda\")  # Move model to GPU explicitly\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Inspect parameter names to determine layer structure\n",
    "print(\"Inspecting model layers to identify transformer blocks...\")\n",
    "sample_param_names = [name for name, _ in list(model.named_parameters())[:50]]\n",
    "for name in sample_param_names:\n",
    "    print(name)\n",
    "\n",
    "# Based on known TinyLlama structure (same as LLaMA), layers are under \"model.layers\"\n",
    "layer_prefix = \"model.layers\"\n",
    "\n",
    "# Identify layer indices\n",
    "layer_nums = sorted(set(\n",
    "    int(name.split(f\"{layer_prefix}.\")[1].split(\".\")[0])\n",
    "    for name, _ in model.named_parameters()\n",
    "    if layer_prefix in name and name.split(f\"{layer_prefix}.\")[1].split(\".\")[0].isdigit()\n",
    "))\n",
    "\n",
    "total_layers = len(layer_nums)\n",
    "layers_to_freeze = int(0.8 * total_layers)\n",
    "\n",
    "print(f\"\\nTotal transformer layers: {total_layers}\")\n",
    "print(f\"Freezing the bottom {layers_to_freeze} layers...\")\n",
    "\n",
    "# Freeze parameters accordingly\n",
    "for name, param in model.named_parameters():\n",
    "    if \"embed_tokens\" in name or \"embed_positions\" in name:\n",
    "        param.requires_grad = False\n",
    "    elif layer_prefix in name:\n",
    "        layer_num = int(name.split(f\"{layer_prefix}.\")[1].split(\".\")[0])\n",
    "        if layer_num < layers_to_freeze:\n",
    "            param.requires_grad = False\n",
    "        else:\n",
    "            param.requires_grad = True\n",
    "            print(f\"Keeping trainable: {name}\")\n",
    "    elif \"lm_head\" in name:\n",
    "        param.requires_grad = True\n",
    "        print(f\"Keeping trainable: {name}\")\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Count trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTrainable parameters: {trainable_params:,}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Percentage trainable: {100 * trainable_params / total_params:.2f}%\")\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset(jsonl_file):\n",
    "    with open(jsonl_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "\n",
    "    system_prompt = \"Summarize the following legal text.\"\n",
    "    texts = []\n",
    "    for item in data:\n",
    "        text = f\"\"\"### Instruction: {system_prompt}\n",
    "\n",
    "### Input:\n",
    "{item['judgement'].strip()[:10000]}\n",
    "\n",
    "### Response:\n",
    "{item['summary'].strip()}\n",
    "\"\"\".strip()\n",
    "        texts.append(text)\n",
    "\n",
    "    dataset = Dataset.from_dict({\"text\": texts})\n",
    "    return dataset\n",
    "\n",
    "# Load dataset\n",
    "train_file = \"full_summaries.jsonl\"\n",
    "train_dataset = load_dataset(train_file)\n",
    "\n",
    "# Set up training parameters\n",
    "train_params = SFTConfig(\n",
    "    output_dir=\"../results_partial_model\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"adamw_torch\",\n",
    "    save_steps=50,\n",
    "    logging_steps=50,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,  # Set True if you want mixed precision\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"tensorboard\",\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=2048,\n",
    "    ddp_find_unused_parameters=False\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    args=train_params\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training (~20% of layers + lm_head)...\")\n",
    "start_time = time.time()\n",
    "trainer.train()\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "# Save the model\n",
    "print(\"Saving the model...\")\n",
    "model.save_pretrained(\"../partial_model_output\")\n",
    "tokenizer.save_pretrained(\"../partial_model_output\")\n",
    "print(\"Model saved at '../partial_model_output'\")\n",
    "\n",
    "# Save training info\n",
    "with open(\"../partial_model_output/training_info.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"training_time_seconds\": training_time,\n",
    "        \"trainable_params\": trainable_params,\n",
    "        \"total_params\": total_params,\n",
    "        \"percentage_trainable\": 100 * trainable_params / total_params\n",
    "    }, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
