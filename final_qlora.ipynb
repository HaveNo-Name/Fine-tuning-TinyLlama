{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc7f3ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel, PeftConfig\n",
    "import evaluate\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6642f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Set model path and load PEFT config\n",
    "peft_model_path = \"output-qlora-latest\"  # <-- Change to your QLoRA output directory\n",
    "config = PeftConfig.from_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d778b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc118b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Setup 4-bit quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "566a0dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "410c2176",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Btech(H) CS\\Semester-6\\MLOPS and LLM's\\Final_Project\\llm-env\\Lib\\site-packages\\peft\\tuners\\lora\\bnb.py:351: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5: Load LoRA weights into quantized base model\n",
    "model = PeftModel.from_pretrained(base_model, peft_model_path)\n",
    "model = model.merge_and_unload()  # Merges LoRA into the base model for inference\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8363799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 6: Move model to CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c5730b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Load ROUGE metric\n",
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92bfdd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Load test dataset\n",
    "def load_test_dataset(jsonl_file, max_input_length=1024, max_samples=None):\n",
    "    system_prompt = \"Summarize the following legal text.\"\n",
    "    inputs, references = [], []\n",
    "\n",
    "    with open(jsonl_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if max_samples and i >= max_samples:\n",
    "                break\n",
    "            item = json.loads(line)\n",
    "            judgement = item[\"judgement\"].strip()[:max_input_length]\n",
    "            summary = item[\"summary\"].strip()\n",
    "            prompt = f\"\"\"### Instruction: {system_prompt}\n",
    "\n",
    "### Input:\n",
    "{judgement}\n",
    "\n",
    "### Response:\"\"\"\n",
    "            inputs.append(prompt)\n",
    "            references.append(summary)\n",
    "    return inputs, references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65bf97f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Set test file path\n",
    "test_jsonl_path = r\"processed-IN-Abs/test-data/full_summaries.jsonl\"\n",
    "test_inputs, test_references = load_test_dataset(test_jsonl_path, max_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "066c1fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Generate summaries\n",
    "def generate_summary(text, max_new_tokens=256):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=2048).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return output_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5ca746a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 10/10 [01:16<00:00,  7.62s/it]\n"
     ]
    }
   ],
   "source": [
    "# Step 11: Inference loop\n",
    "predictions = []\n",
    "\n",
    "start_time = time.time()\n",
    "for inp in tqdm(test_inputs, desc=\"Running Inference\"):\n",
    "    pred = generate_summary(inp)\n",
    "    predictions.append(pred)\n",
    "inference_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d936e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### Instruction: Summarize the following legal text.\\n\\n### Input:\\nAppeal No. 251 of 1963.\\nAppeal by special leave from the judgment and order dated March 20, 1957, of the Patna High Court in Civil Revision No. 40 of 1956.\\nM. C. Setalvad, and R. C. Prasad, for the appellants.\\nThe respondent did not appear.\\nMarch 24, 1964.\\nThe short question which arises in this appeal is whether the term \"wages\" as defined by section 2(vi) of the (No. 4 of 1936) (hereinafter called \\'the Act \\') includes wages fixed by an award in an industrial dispute between the employer and his employees.\\nThis question has to be answered in the light of the definition prescribed by section 2(vi) before it was amended in 1958.\\nThe subsequent amendment expressly provides by section 2(vi) (a) that any remuneration payable under any award or settlement between the parties or order of a Court, would be included in the main definition under section 2(vi).\\nThe point which we have to decide in the present appeal is whether the remuneration payable under an award was not already included in the definition of wages b\\n\\n### Response:\\nThe term \"wages\" as defined by section 2(vi) of the Act includes wages fixed by an award in an industrial dispute between the employer and his employees.\\nThe amendment made in 1958 to section 2(vi) (a) by section 2(vi) (b) of the Act, which provides that any remuneration payable under any award or settlement between the parties or order of a Court would be included in the main definition under section 2(vi), does not affect the interpretation of the term \"wages\" as defined by section 2(vi) of the Act.\\nThe amendment made in 1958 to section 2(vi) (a) does not affect the interpretation of the term \"wages\" as defined by section 2(vi) of the Act.\\nTherefore, the remuneration payable under an award is included in the definition of wages as defined by section 2(vi) of the Act.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5c172e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_result = rouge.compute(predictions=predictions, references=test_references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3b37e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🕒 Inference time for 10 samples: 76.15 seconds\n",
      "\n",
      "📊 ROUGE scores:\n",
      "  rouge1: 0.3427\n",
      "  rouge2: 0.0851\n",
      "  rougeL: 0.1900\n",
      "  rougeLsum: 0.3130\n"
     ]
    }
   ],
   "source": [
    "# Step 13: Print results\n",
    "print(f\"\\n🕒 Inference time for {len(test_inputs)} samples: {inference_time:.2f} seconds\")\n",
    "print(\"\\n📊 ROUGE scores:\")\n",
    "for key, value in rouge_result.items():\n",
    "    print(f\"  {key}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
