{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aab79b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Btech(H) CS\\Semester-6\\MLOPS and LLM's\\Final_Project\\llm-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4aa1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import json\n",
    "\n",
    "def load_dataset(jsonl_file, max_samples=500):\n",
    "    with open(jsonl_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = [json.loads(line) for line in f][:max_samples]\n",
    "\n",
    "    prompt_template = \"### Instruction: Summarize the following legal text.\\n\\n### Input:\\n{input}\\n\\n### Response:\\n{output}\"\n",
    "    samples = []\n",
    "\n",
    "    for item in data:\n",
    "        input_text = item['judgement'].strip()[:10000]\n",
    "        output_text = item['summary'].strip()\n",
    "        full_prompt = prompt_template.format(input=input_text, output=output_text)\n",
    "        samples.append(full_prompt)\n",
    "\n",
    "    return Dataset.from_dict({\"text\": samples})\n",
    "\n",
    "train_path =r\"processed-IN-Abs\\train-data\\full_summaries.jsonl\"\n",
    "train_dataset = load_dataset(train_path, max_samples=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88061cff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94c3b16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1a186d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb840733",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output-lora-latest\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    logging_steps=50,\n",
    "    report_to=\"tensorboard\",\n",
    "    remove_unused_columns=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c5556d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting train dataset to ChatML: 100%|██████████| 500/500 [00:00<00:00, 45452.91 examples/s]\n",
      "Adding EOS to train dataset: 100%|██████████| 500/500 [00:00<00:00, 26311.42 examples/s]\n",
      "Tokenizing train dataset:   0%|          | 0/500 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2763 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Tokenizing train dataset: 100%|██████████| 500/500 [00:02<00:00, 222.04 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 500/500 [00:00<00:00, 100035.87 examples/s]\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    peft_config=peft_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "268c0b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='310' max='310' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [310/310 1:44:54, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.781500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.655500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.611300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.603500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.578500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.580700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=310, training_loss=1.6333228541958718, metrics={'train_runtime': 6316.388, 'train_samples_per_second': 0.396, 'train_steps_per_second': 0.049, 'total_flos': 1.5669894180962304e+16, 'train_loss': 1.6333228541958718})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44580b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('output-lora-latest\\\\tokenizer_config.json',\n",
       " 'output-lora-latest\\\\special_tokens_map.json',\n",
       " 'output-lora-latest\\\\tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"output-lora-latest\")\n",
    "tokenizer.save_pretrained(\"output-lora-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8fea8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Btech(H) CS\\Semester-6\\MLOPS and LLM's\\Final_Project\\llm-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at output-lora-latest were not used when initializing LlamaForCausalLM: ['model.layers.0.self_attn.q_proj.base_layer.weight', 'model.layers.0.self_attn.q_proj.lora_A.default.weight', 'model.layers.0.self_attn.q_proj.lora_B.default.weight', 'model.layers.0.self_attn.v_proj.base_layer.weight', 'model.layers.0.self_attn.v_proj.lora_A.default.weight', 'model.layers.0.self_attn.v_proj.lora_B.default.weight', 'model.layers.1.self_attn.q_proj.base_layer.weight', 'model.layers.1.self_attn.q_proj.lora_A.default.weight', 'model.layers.1.self_attn.q_proj.lora_B.default.weight', 'model.layers.1.self_attn.v_proj.base_layer.weight', 'model.layers.1.self_attn.v_proj.lora_A.default.weight', 'model.layers.1.self_attn.v_proj.lora_B.default.weight', 'model.layers.10.self_attn.q_proj.base_layer.weight', 'model.layers.10.self_attn.q_proj.lora_A.default.weight', 'model.layers.10.self_attn.q_proj.lora_B.default.weight', 'model.layers.10.self_attn.v_proj.base_layer.weight', 'model.layers.10.self_attn.v_proj.lora_A.default.weight', 'model.layers.10.self_attn.v_proj.lora_B.default.weight', 'model.layers.11.self_attn.q_proj.base_layer.weight', 'model.layers.11.self_attn.q_proj.lora_A.default.weight', 'model.layers.11.self_attn.q_proj.lora_B.default.weight', 'model.layers.11.self_attn.v_proj.base_layer.weight', 'model.layers.11.self_attn.v_proj.lora_A.default.weight', 'model.layers.11.self_attn.v_proj.lora_B.default.weight', 'model.layers.12.self_attn.q_proj.base_layer.weight', 'model.layers.12.self_attn.q_proj.lora_A.default.weight', 'model.layers.12.self_attn.q_proj.lora_B.default.weight', 'model.layers.12.self_attn.v_proj.base_layer.weight', 'model.layers.12.self_attn.v_proj.lora_A.default.weight', 'model.layers.12.self_attn.v_proj.lora_B.default.weight', 'model.layers.13.self_attn.q_proj.base_layer.weight', 'model.layers.13.self_attn.q_proj.lora_A.default.weight', 'model.layers.13.self_attn.q_proj.lora_B.default.weight', 'model.layers.13.self_attn.v_proj.base_layer.weight', 'model.layers.13.self_attn.v_proj.lora_A.default.weight', 'model.layers.13.self_attn.v_proj.lora_B.default.weight', 'model.layers.14.self_attn.q_proj.base_layer.weight', 'model.layers.14.self_attn.q_proj.lora_A.default.weight', 'model.layers.14.self_attn.q_proj.lora_B.default.weight', 'model.layers.14.self_attn.v_proj.base_layer.weight', 'model.layers.14.self_attn.v_proj.lora_A.default.weight', 'model.layers.14.self_attn.v_proj.lora_B.default.weight', 'model.layers.15.self_attn.q_proj.base_layer.weight', 'model.layers.15.self_attn.q_proj.lora_A.default.weight', 'model.layers.15.self_attn.q_proj.lora_B.default.weight', 'model.layers.15.self_attn.v_proj.base_layer.weight', 'model.layers.15.self_attn.v_proj.lora_A.default.weight', 'model.layers.15.self_attn.v_proj.lora_B.default.weight', 'model.layers.16.self_attn.q_proj.base_layer.weight', 'model.layers.16.self_attn.q_proj.lora_A.default.weight', 'model.layers.16.self_attn.q_proj.lora_B.default.weight', 'model.layers.16.self_attn.v_proj.base_layer.weight', 'model.layers.16.self_attn.v_proj.lora_A.default.weight', 'model.layers.16.self_attn.v_proj.lora_B.default.weight', 'model.layers.17.self_attn.q_proj.base_layer.weight', 'model.layers.17.self_attn.q_proj.lora_A.default.weight', 'model.layers.17.self_attn.q_proj.lora_B.default.weight', 'model.layers.17.self_attn.v_proj.base_layer.weight', 'model.layers.17.self_attn.v_proj.lora_A.default.weight', 'model.layers.17.self_attn.v_proj.lora_B.default.weight', 'model.layers.18.self_attn.q_proj.base_layer.weight', 'model.layers.18.self_attn.q_proj.lora_A.default.weight', 'model.layers.18.self_attn.q_proj.lora_B.default.weight', 'model.layers.18.self_attn.v_proj.base_layer.weight', 'model.layers.18.self_attn.v_proj.lora_A.default.weight', 'model.layers.18.self_attn.v_proj.lora_B.default.weight', 'model.layers.19.self_attn.q_proj.base_layer.weight', 'model.layers.19.self_attn.q_proj.lora_A.default.weight', 'model.layers.19.self_attn.q_proj.lora_B.default.weight', 'model.layers.19.self_attn.v_proj.base_layer.weight', 'model.layers.19.self_attn.v_proj.lora_A.default.weight', 'model.layers.19.self_attn.v_proj.lora_B.default.weight', 'model.layers.2.self_attn.q_proj.base_layer.weight', 'model.layers.2.self_attn.q_proj.lora_A.default.weight', 'model.layers.2.self_attn.q_proj.lora_B.default.weight', 'model.layers.2.self_attn.v_proj.base_layer.weight', 'model.layers.2.self_attn.v_proj.lora_A.default.weight', 'model.layers.2.self_attn.v_proj.lora_B.default.weight', 'model.layers.20.self_attn.q_proj.base_layer.weight', 'model.layers.20.self_attn.q_proj.lora_A.default.weight', 'model.layers.20.self_attn.q_proj.lora_B.default.weight', 'model.layers.20.self_attn.v_proj.base_layer.weight', 'model.layers.20.self_attn.v_proj.lora_A.default.weight', 'model.layers.20.self_attn.v_proj.lora_B.default.weight', 'model.layers.21.self_attn.q_proj.base_layer.weight', 'model.layers.21.self_attn.q_proj.lora_A.default.weight', 'model.layers.21.self_attn.q_proj.lora_B.default.weight', 'model.layers.21.self_attn.v_proj.base_layer.weight', 'model.layers.21.self_attn.v_proj.lora_A.default.weight', 'model.layers.21.self_attn.v_proj.lora_B.default.weight', 'model.layers.3.self_attn.q_proj.base_layer.weight', 'model.layers.3.self_attn.q_proj.lora_A.default.weight', 'model.layers.3.self_attn.q_proj.lora_B.default.weight', 'model.layers.3.self_attn.v_proj.base_layer.weight', 'model.layers.3.self_attn.v_proj.lora_A.default.weight', 'model.layers.3.self_attn.v_proj.lora_B.default.weight', 'model.layers.4.self_attn.q_proj.base_layer.weight', 'model.layers.4.self_attn.q_proj.lora_A.default.weight', 'model.layers.4.self_attn.q_proj.lora_B.default.weight', 'model.layers.4.self_attn.v_proj.base_layer.weight', 'model.layers.4.self_attn.v_proj.lora_A.default.weight', 'model.layers.4.self_attn.v_proj.lora_B.default.weight', 'model.layers.5.self_attn.q_proj.base_layer.weight', 'model.layers.5.self_attn.q_proj.lora_A.default.weight', 'model.layers.5.self_attn.q_proj.lora_B.default.weight', 'model.layers.5.self_attn.v_proj.base_layer.weight', 'model.layers.5.self_attn.v_proj.lora_A.default.weight', 'model.layers.5.self_attn.v_proj.lora_B.default.weight', 'model.layers.6.self_attn.q_proj.base_layer.weight', 'model.layers.6.self_attn.q_proj.lora_A.default.weight', 'model.layers.6.self_attn.q_proj.lora_B.default.weight', 'model.layers.6.self_attn.v_proj.base_layer.weight', 'model.layers.6.self_attn.v_proj.lora_A.default.weight', 'model.layers.6.self_attn.v_proj.lora_B.default.weight', 'model.layers.7.self_attn.q_proj.base_layer.weight', 'model.layers.7.self_attn.q_proj.lora_A.default.weight', 'model.layers.7.self_attn.q_proj.lora_B.default.weight', 'model.layers.7.self_attn.v_proj.base_layer.weight', 'model.layers.7.self_attn.v_proj.lora_A.default.weight', 'model.layers.7.self_attn.v_proj.lora_B.default.weight', 'model.layers.8.self_attn.q_proj.base_layer.weight', 'model.layers.8.self_attn.q_proj.lora_A.default.weight', 'model.layers.8.self_attn.q_proj.lora_B.default.weight', 'model.layers.8.self_attn.v_proj.base_layer.weight', 'model.layers.8.self_attn.v_proj.lora_A.default.weight', 'model.layers.8.self_attn.v_proj.lora_B.default.weight', 'model.layers.9.self_attn.q_proj.base_layer.weight', 'model.layers.9.self_attn.q_proj.lora_A.default.weight', 'model.layers.9.self_attn.q_proj.lora_B.default.weight', 'model.layers.9.self_attn.v_proj.base_layer.weight', 'model.layers.9.self_attn.v_proj.lora_A.default.weight', 'model.layers.9.self_attn.v_proj.lora_B.default.weight']\n",
      "- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at output-lora-latest and are newly initialized: ['model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Inference: 100%|██████████| 10/10 [12:31<00:00, 75.10s/it]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Predictions and/or references don't match the expected format.\nExpected format:\nFeature option 0: {'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id='references')}\nFeature option 1: {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')},\nInput predictions: ['###', 'Instruction:', 'Summarize', ..., 'appel', '###', 'Response:blattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblatt'],\nInput references: [['The', 'appellants', 'who', 'are', 'displaced', 'persons', 'from', 'West', 'Pakistan,', 'were', 'granted', 'quasi', 'permanent', 'allotment', 'of', 'some', 'lands', 'in', 'village', 'Raikot', 'in', '1949.', 'On', 'October', '31,', '1952,', 'the', 'Assistant', 'Custodian', 'cancelled', 'the', 'allotment', 'of', '14', 'allottees', 'in', 'village', 'Karodian,', 'and', 'also', 'cancelled', 'the', 'allotment', 'of', 'the', 'Appellants', 'in', 'Raikot', 'but', 'allotted', 'lands', 'to', 'them', 'in', 'village', 'Karodian,', 'and', 'allotted', 'the', 'lands', 'of', 'Raikot', 'to', 'other', 'persons.', 'The', '14', 'allottees', 'of', 'village', 'Karodian', 'as', 'well', 'as', 'the', 'appellants', 'applied', 'for', 'review', 'of', 'the', 'orders', 'of', 'cancellation', 'of', 'their', 'allotment.', 'The', 'application', 'of', 'the', '14', 'allottees', 'was', 'dismissed.', 'They', 'preferred', 'a', 'revision', 'to', 'the', 'Custodian', 'General', 'who', 'cancelled', 'the', 'appellant', \"'s\", 'allotment', '(1)', 'Cal.', '926.', '329', 'in', 'Karodian', 'and', 'restored', 'the', 'allotment', 'of', 'the', '14', 'allottees', 'on', 'December', '17,', '1954', 'Thereupon,,', 'on', 'January', '6,', '1955,', 'the', 'appellants', 'moved', 'the', 'Custodian', 'General', 'for', 'calling', 'up', 'their', 'review', 'application', 'and', 'for', 'revising', 'the', 'order', 'of', 'October', '31,', '1952,', 'cancelling', 'their', 'allotment', 'in', 'Raikot.', 'The', 'Custodian', 'General', 'refused', 'to', 'revise', 'the', 'order', 'on', 'the', 'ground', 'that', 'his', 'power', 'to', 'revise', 'had', 'been', 'taken', 'away', 'by', 'the', '.', 'The', 'appellants', 'contended', 'that', 'the,', 'Custodian', 'General', 'had', 'the', 'power', 'to', 'revise', 'the', 'order.', 'Held,', 'that', 'after', 'the', 'enactment', 'of', 'the,', ',', 'the', 'Custodian', 'General', 'ceased', 'to', 'have', 'the', 'power', 'to', 'cancel', 'allotments.', 'By,', 'the', 'issuing', 'of', 'a', 'notification', 'under,', 'section', '12(1)', 'of', 'this', 'Act,', 'the', 'Fight,', 'title', 'or', 'interest', 'of', 'the', 'evacuee', 'in', 'the', 'property', 'specified', 'in', 'the', 'notification', 'was', 'extinguished', 'and', 'the', 'property', 'vested', 'absolutely', 'in', 'the', 'Central.', 'Government.', 'The', 'right', 'of', 'the', 'Custodian', 'manage', 'the', 'property', 'under', 'the', ',', 'came', 'to', 'an', 'end', 'and', 'the', 'management', 'vested', 'in', 'a', 'new', 'set', 'of', 'officers.', 'Even', 'though', 'no', 'managing', 'officer', 'was', 'appointed', 'or', 'a', 'managing', 'corporation,', 'constituted', 'under', 'the', 'new', 'Act', 'to', 'manage', 'the', 'property', 'no', 'one', 'else', 'could', \"'exercise\", 'the', 'power', 'of', 'cancellation', 'of', 'allotment.', 'Bal', 'Mukund', 'vs', 'The', 'State', 'of', 'Punjab,', 'I.L.R.', '1957', 'Punj.', '712,', 'approved.']]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 78\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# Compute BLEU (expects tokenized predictions and references)\u001b[39;00m\n\u001b[32m     77\u001b[39m tokenized_preds = [pred.split() \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m predictions]\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m bleu_result = \u001b[43mbleu\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenized_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreferences_for_bleu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInference time for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_inputs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minference_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     81\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mROUGE scores:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Btech(H) CS\\Semester-6\\MLOPS and LLM's\\Final_Project\\llm-env\\Lib\\site-packages\\evaluate\\module.py:455\u001b[39m, in \u001b[36mEvaluationModule.compute\u001b[39m\u001b[34m(self, predictions, references, **kwargs)\u001b[39m\n\u001b[32m    452\u001b[39m compute_kwargs = {k: kwargs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._feature_names()}\n\u001b[32m    454\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m inputs.values()):\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[38;5;28mself\u001b[39m._finalize()\n\u001b[32m    458\u001b[39m \u001b[38;5;28mself\u001b[39m.cache_file_name = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Btech(H) CS\\Semester-6\\MLOPS and LLM's\\Final_Project\\llm-env\\Lib\\site-packages\\evaluate\\module.py:514\u001b[39m, in \u001b[36mEvaluationModule.add_batch\u001b[39m\u001b[34m(self, predictions, references, **kwargs)\u001b[39m\n\u001b[32m    512\u001b[39m batch = {input_name: batch[input_name] \u001b[38;5;28;01mfor\u001b[39;00m input_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._feature_names()}\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m514\u001b[39m     \u001b[38;5;28mself\u001b[39m.selected_feature_format = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_infer_feature_from_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    515\u001b[39m     \u001b[38;5;28mself\u001b[39m._init_writer()\n\u001b[32m    516\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Btech(H) CS\\Semester-6\\MLOPS and LLM's\\Final_Project\\llm-env\\Lib\\site-packages\\evaluate\\module.py:596\u001b[39m, in \u001b[36mEvaluationModule._infer_feature_from_batch\u001b[39m\u001b[34m(self, batch)\u001b[39m\n\u001b[32m    594\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    595\u001b[39m     example = \u001b[38;5;28mdict\u001b[39m([(k, v[\u001b[32m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch.items()])\n\u001b[32m--> \u001b[39m\u001b[32m596\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_infer_feature_from_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Btech(H) CS\\Semester-6\\MLOPS and LLM's\\Final_Project\\llm-env\\Lib\\site-packages\\evaluate\\module.py:616\u001b[39m, in \u001b[36mEvaluationModule._infer_feature_from_example\u001b[39m\u001b[34m(self, example)\u001b[39m\n\u001b[32m    609\u001b[39m feature_strings = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join([\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFeature option \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, feature \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.features)])\n\u001b[32m    610\u001b[39m error_msg = (\n\u001b[32m    611\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPredictions and/or references don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt match the expected format.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    612\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected format:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfeature_strings\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    613\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInput predictions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummarize_if_long_list(example[\u001b[33m'\u001b[39m\u001b[33mpredictions\u001b[39m\u001b[33m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    614\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInput references: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummarize_if_long_list(example[\u001b[33m'\u001b[39m\u001b[33mreferences\u001b[39m\u001b[33m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    615\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Predictions and/or references don't match the expected format.\nExpected format:\nFeature option 0: {'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id='references')}\nFeature option 1: {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')},\nInput predictions: ['###', 'Instruction:', 'Summarize', ..., 'appel', '###', 'Response:blattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblattblatt'],\nInput references: [['The', 'appellants', 'who', 'are', 'displaced', 'persons', 'from', 'West', 'Pakistan,', 'were', 'granted', 'quasi', 'permanent', 'allotment', 'of', 'some', 'lands', 'in', 'village', 'Raikot', 'in', '1949.', 'On', 'October', '31,', '1952,', 'the', 'Assistant', 'Custodian', 'cancelled', 'the', 'allotment', 'of', '14', 'allottees', 'in', 'village', 'Karodian,', 'and', 'also', 'cancelled', 'the', 'allotment', 'of', 'the', 'Appellants', 'in', 'Raikot', 'but', 'allotted', 'lands', 'to', 'them', 'in', 'village', 'Karodian,', 'and', 'allotted', 'the', 'lands', 'of', 'Raikot', 'to', 'other', 'persons.', 'The', '14', 'allottees', 'of', 'village', 'Karodian', 'as', 'well', 'as', 'the', 'appellants', 'applied', 'for', 'review', 'of', 'the', 'orders', 'of', 'cancellation', 'of', 'their', 'allotment.', 'The', 'application', 'of', 'the', '14', 'allottees', 'was', 'dismissed.', 'They', 'preferred', 'a', 'revision', 'to', 'the', 'Custodian', 'General', 'who', 'cancelled', 'the', 'appellant', \"'s\", 'allotment', '(1)', 'Cal.', '926.', '329', 'in', 'Karodian', 'and', 'restored', 'the', 'allotment', 'of', 'the', '14', 'allottees', 'on', 'December', '17,', '1954', 'Thereupon,,', 'on', 'January', '6,', '1955,', 'the', 'appellants', 'moved', 'the', 'Custodian', 'General', 'for', 'calling', 'up', 'their', 'review', 'application', 'and', 'for', 'revising', 'the', 'order', 'of', 'October', '31,', '1952,', 'cancelling', 'their', 'allotment', 'in', 'Raikot.', 'The', 'Custodian', 'General', 'refused', 'to', 'revise', 'the', 'order', 'on', 'the', 'ground', 'that', 'his', 'power', 'to', 'revise', 'had', 'been', 'taken', 'away', 'by', 'the', '.', 'The', 'appellants', 'contended', 'that', 'the,', 'Custodian', 'General', 'had', 'the', 'power', 'to', 'revise', 'the', 'order.', 'Held,', 'that', 'after', 'the', 'enactment', 'of', 'the,', ',', 'the', 'Custodian', 'General', 'ceased', 'to', 'have', 'the', 'power', 'to', 'cancel', 'allotments.', 'By,', 'the', 'issuing', 'of', 'a', 'notification', 'under,', 'section', '12(1)', 'of', 'this', 'Act,', 'the', 'Fight,', 'title', 'or', 'interest', 'of', 'the', 'evacuee', 'in', 'the', 'property', 'specified', 'in', 'the', 'notification', 'was', 'extinguished', 'and', 'the', 'property', 'vested', 'absolutely', 'in', 'the', 'Central.', 'Government.', 'The', 'right', 'of', 'the', 'Custodian', 'manage', 'the', 'property', 'under', 'the', ',', 'came', 'to', 'an', 'end', 'and', 'the', 'management', 'vested', 'in', 'a', 'new', 'set', 'of', 'officers.', 'Even', 'though', 'no', 'managing', 'officer', 'was', 'appointed', 'or', 'a', 'managing', 'corporation,', 'constituted', 'under', 'the', 'new', 'Act', 'to', 'manage', 'the', 'property', 'no', 'one', 'else', 'could', \"'exercise\", 'the', 'power', 'of', 'cancellation', 'of', 'allotment.', 'Bal', 'Mukund', 'vs', 'The', 'State', 'of', 'Punjab,', 'I.L.R.', '1957', 'Punj.', '712,', 'approved.']]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load your fine-tuned model and tokenizer\n",
    "model_dir = \"output-lora-latest\"  # <-- update with your saved model path\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_dir)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Load ROUGE and BLEU metrics from 'evaluate' library\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "# Load and preprocess test dataset from jsonl, with optional max_samples limit\n",
    "def load_test_dataset(jsonl_file, max_input_length=1024, max_samples=None):\n",
    "    system_prompt = \"Summarize the following legal text.\"\n",
    "    inputs = []\n",
    "    references = []\n",
    "    with open(jsonl_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if max_samples and i >= max_samples:\n",
    "                break\n",
    "            item = json.loads(line)\n",
    "            input_text = f\"\"\"### Instruction: {system_prompt}\n",
    "\n",
    "### Input:\n",
    "{item['judgement'].strip()[:max_input_length]}\n",
    "\n",
    "### Response:\"\"\"\n",
    "            inputs.append(input_text)\n",
    "            references.append(item['summary'].strip())\n",
    "    return inputs, references\n",
    "\n",
    "# Path to your test set jsonl file\n",
    "test_jsonl_path = r\"processed-IN-Abs\\test-data\\full_summaries.jsonl\"  # <-- replace with your path\n",
    "\n",
    "# Load only first 10 examples (change or set None for full test set)\n",
    "test_inputs, test_references = load_test_dataset(test_jsonl_path, max_samples=10)\n",
    "\n",
    "# Function to generate summary from input text\n",
    "def generate_summary(text, max_new_tokens=256):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=2048).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,  # generate tokens beyond input length\n",
    "            do_sample=False,  # greedy decoding\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# Generate predictions and collect references for metrics\n",
    "predictions = []\n",
    "references_for_bleu = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for inp, ref in tqdm(zip(test_inputs, test_references), total=len(test_inputs), desc=\"Inference\"):\n",
    "    pred = generate_summary(inp)\n",
    "    predictions.append(pred)\n",
    "    references_for_bleu.append([ref.split()])  # BLEU expects list of tokenized references\n",
    "\n",
    "inference_time = time.time() - start_time\n",
    "\n",
    "# Compute ROUGE (expects raw strings)\n",
    "rouge_result = rouge.compute(predictions=predictions, references=test_references)\n",
    "\n",
    "# Compute BLEU (expects tokenized predictions and references)\n",
    "tokenized_preds = [pred.split() for pred in predictions]\n",
    "bleu_result = bleu.compute(predictions=tokenized_preds, references=references_for_bleu)\n",
    "\n",
    "print(f\"Inference time for {len(test_inputs)} samples: {inference_time:.2f} seconds\")\n",
    "print(\"\\nROUGE scores:\")\n",
    "for key, value in rouge_result.items():\n",
    "    print(f\"  {key}: {value.mid.fmeasure:.4f}\")\n",
    "\n",
    "print(f\"\\nBLEU score: {bleu_result['bleu']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a417280a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "import evaluate\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23a0a101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load LoRA configuration to get base model name\n",
    "peft_model_path = \"output-lora-latest\\checkpoint-310\"  # <-- change to your LoRA output directory\n",
    "config = PeftConfig.from_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "270ff264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load base model and tokenizer\n",
    "base_model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9da95589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Merge LoRA weights into the base model\n",
    "model = PeftModel.from_pretrained(base_model, peft_model_path)\n",
    "model = model.merge_and_unload()  # Important for correct weights\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6272e826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Move model to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f8ff550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Load metrics\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a150f4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_dataset(jsonl_file, max_input_length=1024, max_samples=None):\n",
    "    system_prompt = \"Summarize the following legal text.\"\n",
    "    inputs, references = [], []\n",
    "\n",
    "    with open(jsonl_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if max_samples and i >= max_samples:\n",
    "                break\n",
    "            item = json.loads(line)\n",
    "            judgement = item[\"judgement\"].strip()[:max_input_length]\n",
    "            summary = item[\"summary\"].strip()\n",
    "            prompt = f\"\"\"### Instruction: {system_prompt}\n",
    "\n",
    "### Input:\n",
    "{judgement}\n",
    "\n",
    "### Response:\"\"\"\n",
    "            inputs.append(prompt)\n",
    "            references.append(summary)\n",
    "    return inputs, references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "191f4485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Set test file path\n",
    "test_jsonl_path = r\"processed-IN-Abs/test-data/full_summaries.jsonl\"  # <-- update path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66ce8348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load only first 10 examples for quick evaluation (set to None for full test)\n",
    "test_inputs, test_references = load_test_dataset(test_jsonl_path, max_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82fd904d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Generate summary\n",
    "def generate_summary(text, max_new_tokens=256):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=2048).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return output_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50bd9928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Inference loop\n",
    "predictions = []\n",
    "references_for_bleu = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2556aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 10/10 [02:13<00:00, 13.37s/it]\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for inp, ref in tqdm(zip(test_inputs, test_references), total=len(test_inputs), desc=\"Running Inference\"):\n",
    "    pred = generate_summary(inp)\n",
    "    predictions.append(pred)\n",
    "    references_for_bleu.append([ref.split()])  # BLEU expects tokenized reference list\n",
    "\n",
    "inference_time = time.time() - start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a60eca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### Instruction: Summarize the following legal text.\\n\\n### Input:\\nAppeal No. 251 of 1963.\\nAppeal by special leave from the judgment and order dated March 20, 1957, of the Patna High Court in Civil Revision No. 40 of 1956.\\nM. C. Setalvad, and R. C. Prasad, for the appellants.\\nThe respondent did not appear.\\nMarch 24, 1964.\\nThe short question which arises in this appeal is whether the term \"wages\" as defined by section 2(vi) of the (No. 4 of 1936) (hereinafter called \\'the Act \\') includes wages fixed by an award in an industrial dispute between the employer and his employees.\\nThis question has to be answered in the light of the definition prescribed by section 2(vi) before it was amended in 1958.\\nThe subsequent amendment expressly provides by section 2(vi) (a) that any remuneration payable under any award or settlement between the parties or order of a Court, would be included in the main definition under section 2(vi).\\nThe point which we have to decide in the present appeal is whether the remuneration payable under an award was not already included in the definition of wages b\\n\\n### Response:\\nThe definition of wages in section 2(vi) of the Act is as follows: \"wages\" means the remuneration payable to an employee in respect of his employment, whether or not such remuneration is paid in cash or in kind, and includes any allowance or benefit in kind, but does not include any remuneration payable under any award or settlement between the parties or order of a Court.\\nThe word \"wages\" is defined in section 2(vi) of the Act.\\nThe word \"wages\" is defined in section 2(vi) of the Act.\\nThe word \"wages\" is defined in section 2(vi) of the Act.\\nThe word \"wages\" is defined in section 2(vi) of the Act.\\nThe word \"wages\" is defined in section 2(vi) of the Act.\\nThe word \"wages\" is defined in section 2(vi) of the Act.\\nThe word \"wages\" is defined in section 2(vi) of the Act.\\nThe word \"wages\" is defined in section 2(vi) of the Act.\\nThe'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e26f6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Evaluate with ROUGE\n",
    "rouge_result = rouge.compute(predictions=predictions, references=test_references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aab9977c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🕒 Inference time for 10 samples: 133.75 seconds\n",
      "\n",
      "📊 ROUGE scores:\n",
      "  rouge1: 0.3469\n",
      "  rouge2: 0.0858\n",
      "  rougeL: 0.1819\n",
      "  rougeLsum: 0.3182\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n🕒 Inference time for {len(test_inputs)} samples: {inference_time:.2f} seconds\")\n",
    "print(\"\\n📊 ROUGE scores:\")\n",
    "for key, value in rouge_result.items():\n",
    "    print(f\"  {key}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
