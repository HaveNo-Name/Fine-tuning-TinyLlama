{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b110015",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Btech(H) CS\\Semester-6\\MLOPS and LLM's\\Final_Project\\llm-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import evaluate\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c37529f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"final_layer_finetune\"  # Update this to your saved model directory\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d91b71c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=2048, out_features=256, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b9a65af",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06e816ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_dataset(jsonl_file, max_input_length=1024, max_samples=None):\n",
    "    system_prompt = \"Summarize the following legal text.\"\n",
    "    inputs, references = [], []\n",
    "\n",
    "    with open(jsonl_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if max_samples and i >= max_samples:\n",
    "                break\n",
    "            item = json.loads(line)\n",
    "            judgement = item[\"judgement\"].strip()[:max_input_length]\n",
    "            summary = item[\"summary\"].strip()\n",
    "            prompt = f\"\"\"### Instruction: {system_prompt}\n",
    "\n",
    "### Input:\n",
    "{judgement}\n",
    "\n",
    "### Response:\"\"\"\n",
    "            inputs.append(prompt)\n",
    "            references.append(summary)\n",
    "    return inputs, references\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aba32bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_jsonl_path = r\"processed-IN-Abs/test-data/full_summaries.jsonl\"  # Update this to your test dataset path\n",
    "test_inputs, test_references = load_test_dataset(test_jsonl_path, max_samples=10)  # change max_samples=None for full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a900580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(text, max_new_tokens=256):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=2048).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "350559ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [12:15<00:00, 73.60s/it]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "start_time = time.time()\n",
    "\n",
    "for inp in tqdm(test_inputs, desc=\"Running Inference\"):\n",
    "    pred = generate_summary(inp)\n",
    "    predictions.append(pred)\n",
    "\n",
    "inference_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1f8bf53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### Instruction: Summarize the following legal text.\\n\\n### Input:\\nAppeal No. 251 of 1963.\\nAppeal by special leave from the judgment and order dated March 20, 1957, of the Patna High Court in Civil Revision No. 40 of 1956.\\nM. C. Setalvad, and R. C. Prasad, for the appellants.\\nThe respondent did not appear.\\nMarch 24, 1964.\\nThe short question which arises in this appeal is whether the term \"wages\" as defined by section 2(vi) of the (No. 4 of 1936) (hereinafter called \\'the Act \\') includes wages fixed by an award in an industrial dispute between the employer and his employees.\\nThis question has to be answered in the light of the definition prescribed by section 2(vi) before it was amended in 1958.\\nThe subsequent amendment expressly provides by section 2(vi) (a) that any remuneration payable under any award or settlement between the parties or order of a Court, would be included in the main definition under section 2(vi).\\nThe point which we have to decide in the present appeal is whether the remuneration payable under an award was not already included in the definition of wages b\\n\\n### Response:\\nThe definition of wages in section 2(vi) of the Act includes wages fixed by an award in an industrial dispute between the employer and his employees. The amendment in 1958 expressly provided that any remuneration payable under any award or settlement between the parties or order of a Court would be included in the main definition under section 2(vi). The definition of wages in section 2(vi) of the Act is not limited to wages fixed by an award. It includes wages fixed by an award or settlement between the parties or order of a Court. The amendment in 1958 expressly provided that any remuneration payable under any award or settlement between the parties or order of a Court would be included in the main definition under section 2(vi). Therefore, the remuneration payable under an award is included in the definition of wages under section 2(vi) of the Act.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e7726ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_result = rouge.compute(predictions=predictions, references=test_references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9735b5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ•’ Inference time for 10 samples: 735.97 seconds\n",
      "\n",
      "ðŸ“Š ROUGE scores:\n",
      "  rouge1: 0.3216\n",
      "  rouge2: 0.0843\n",
      "  rougeL: 0.1721\n",
      "  rougeLsum: 0.2972\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nðŸ•’ Inference time for {len(test_inputs)} samples: {inference_time:.2f} seconds\")\n",
    "print(\"\\nðŸ“Š ROUGE scores:\")\n",
    "for key, value in rouge_result.items():\n",
    "    print(f\"  {key}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
